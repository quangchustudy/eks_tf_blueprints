# Team Management
git push --set-upstream origin features/env_vpc_eks_module

## Add Platform Team
The first thing we need to do is add the Platform Team definition to our main.tf in the module eks_blueprints. This is the team that manages the EKS cluster provisioning.
```bash
cat <<'EOF' >> ./environment/eks-blueprint/modules/eks_cluster/main.tf

data "aws_iam_role" "eks_admin_role_name" {
  count     = local.eks_admin_role_name != "" ? 1 : 0
  name = local.eks_admin_role_name
}

module "eks_blueprints_platform_teams" {
  source  = "aws-ia/eks-blueprints-teams/aws"
  version = "~> 0.2"

  name = "team-platform"

  # Enables elevated, admin privileges for this team
  enable_admin = true
 
  # Define who can impersonate the team-platform Role
  users             = [
    data.aws_caller_identity.current.arn,
    try(data.aws_iam_role.eks_admin_role_name[0].arn, data.aws_caller_identity.current.arn),
  ]
  cluster_arn       = module.eks.cluster_arn
  oidc_provider_arn = module.eks.oidc_provider_arn

  labels = {
    "elbv2.k8s.aws/pod-readiness-gate-inject" = "enabled",
    "appName"                                 = "platform-team-app",
    "projectName"                             = "project-platform",
  }

  annotations = {
    team = "platform"
  }

  namespaces = {
    "team-platform" = {

      resource_quota = {
        hard = {
          "requests.cpu"    = "10000m",
          "requests.memory" = "20Gi",
          "limits.cpu"      = "20000m",
          "limits.memory"   = "50Gi",
          "pods"            = "20",
          "secrets"         = "20",
          "services"        = "20"
        }
      }

      limit_range = {
        limit = [
          {
            type = "Pod"
            max = {
              cpu    = "1000m"
              memory = "1Gi"
            },
            min = {
              cpu    = "10m"
              memory = "4Mi"
            }
          },
          {
            type = "PersistentVolumeClaim"
            min = {
              storage = "24M"
            }
          }
        ]
      }
    }

  }

  tags = local.tags
}
EOF

```

team-platform will be admin of the EKS cluster, so we activate this option on line 15. The module will create a new IAM Role and we define on line 18 which other entities (AWS Users or Roles) will be able to impersonate this role and gain admin access on the cluster. For this, we reuse the local configuration from our module variable to specify the additional IAM Role.

We also want our platform-team to own a dedicated Kubernetes namespace so that they can deploy some cluster-level Kubernetes objects, like Network policies, Security control manifests, Autoscaling configuration, etc. In line 35, we create the team-platform namespace, and because this is a shared EKS cluster, we create resources_quota (line 38) for this namespace and a limit_range (line 50) object.

Important
Don't forget to save the cloud9 file, as auto-save is not enabled by default.

### 2. Execute it
```bash
# We need to do this again since we added a new module.
cd ./environment/eks-blueprint/eks-blue
terraform init
# It is always a good practice to use a dry-run command
terraform plan
# then provision our EKS cluster
# the auto approve flag avoids you having to confirm you want to provision resources.
terraform apply -auto-approve

```

This will create a dedicated role similar to arn:aws:iam::0123456789:role/team-platform-XXXXXXXXXXXX that will allow you to manage the cluster as an administrator.

It also defines which existing users/roles will be allowed to assume this role via the users parameter, where you can provide a list of IAM arns. The new role is also configured in the EKS Ã ws-auth` Configmap to allow authentication into the EKS Kubernetes cluster.

When Terraform finishes its run, you can explore the new objects it created.

We can see, for instance, that a new namespace has been created:

```bash
kubectl get ns

```
We can see the resource quotas allowed for this new namespace, and if we run:
```bash
kubectl describe resourcequotas -n team-platform

```
We will see the limit-range configuration that has been applied, allowing us to add default resources/limits quotas to our applications.
```bash
kubectl describe limitrange -n team-platform

```

There are several other resources created when you onboard a team, including a Kubernetes Service Account created for the team. This service account can also be used by our applications deployed into this namespace to inherit the IAM permissions associated with the IAM Role associated; you can see it with the special annotation eks.amazonaws.com/role-arn

```bash
kubectl describe sa -n team-platform team-platform

```

We can also explore the Terraform state files generated by our apply using terraform state list and you should see resources similar to the ones shown below:

```bash
terraform state list module.eks_cluster.module.eks_blueprints_platform_teams

```

You can see in more detail in the terraform state what AWS resources were created with our team module. For example, you can see the platform team details:

```bash
terraform state show 'module.eks_cluster.module.eks_blueprints_platform_teams.aws_iam_role.this[0]'

```